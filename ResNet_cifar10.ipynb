{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ResNet_cifar10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOIdaMWbPf/0eJf79B/yQrH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/seowon98/DeepLearning/blob/master/ResNet_cifar10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UPnTzSydFXDh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xuy7gEuLGFoW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import visdom\n",
        "\n",
        "vis = visdom.Visdom()\n",
        "vis.close(env=\"main\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lAlirD7tF87H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def value_tracker(value_plot, value, num):\n",
        "    '''num, loss_value, are Tensor'''\n",
        "    vis.line(X=num,\n",
        "             Y=value,\n",
        "             win = value_plot,\n",
        "             update='append'\n",
        "             )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZcs1qSKGGPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "torch.manual_seed(777)\n",
        "if device =='cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_bJ19IjIXiS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "3600cc2f-cab1-41db-dd6c-cef7b7964cbe"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cuda'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GLf4a_nGNV9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "8daf83b8-13fc-47f4-c0f9-8ac418fc8a15"
      },
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True, download=True, transform=transform)\n",
        "\n",
        "print(trainset.data.shape)\n",
        "\n",
        "train_data_mean = trainset.data.mean( axis=(0,1,2) )\n",
        "train_data_std = trainset.data.std( axis=(0,1,2) )\n",
        "\n",
        "\n",
        "print(train_data_mean)\n",
        "print(train_data_std)\n",
        "\n",
        "train_data_mean = train_data_mean / 255\n",
        "train_data_std = train_data_std / 255\n",
        "\n",
        "print(train_data_mean)\n",
        "print(train_data_std)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "(50000, 32, 32, 3)\n",
            "[125.30691805 122.95039414 113.86538318]\n",
            "[62.99321928 62.08870764 66.70489964]\n",
            "[0.49139968 0.48215841 0.44653091]\n",
            "[0.24703223 0.24348513 0.26158784]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRYTEueEGTVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "4b2e1ac3-b087-4b9d-d8e3-30144b4037a8"
      },
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(train_data_mean, train_data_std)\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(train_data_mean, train_data_std)\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n",
        "                                        download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=256,\n",
        "                                          shuffle=True, num_workers=0)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n",
        "                                       download=True, transform=transform_test)\n",
        "\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=256,\n",
        "                                         shuffle=False, num_workers=0)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat',\n",
        "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oSJqjZvmGZUS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models.resnet as resnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_Pt_sJ9Ga5z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "conv1x1=resnet.conv1x1\n",
        "Bottleneck = resnet.Bottleneck\n",
        "BasicBlock= resnet.BasicBlock"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VuqqcUqoGcOU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        #self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "        \n",
        "        self.layer1 = self._make_layer(block, 16, layers[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=1)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 128, layers[3], stride=2)\n",
        "        \n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(128 * block.expansion, num_classes)\n",
        "\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "        # Zero-initialize the last BN in each residual branch,\n",
        "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
        "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
        "        if zero_init_residual:\n",
        "            for m in self.modules():\n",
        "                if isinstance(m, Bottleneck):\n",
        "                    nn.init.constant_(m.bn3.weight, 0)\n",
        "                elif isinstance(m, BasicBlock):\n",
        "                    nn.init.constant_(m.bn2.weight, 0)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride, downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        #x.shape =[1, 16, 32,32]\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        #x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        #x.shape =[1, 128, 32,32]\n",
        "        x = self.layer2(x)\n",
        "        #x.shape =[1, 256, 32,32]\n",
        "        x = self.layer3(x)\n",
        "        #x.shape =[1, 512, 16,16]\n",
        "        x = self.layer4(x)\n",
        "        #x.shape =[1, 1024, 8,8]\n",
        "        \n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTWEKVLjGe3B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "resnet50 = ResNet(resnet.Bottleneck, [3, 4, 6, 3], 10, True).to(device) \n",
        "#1(conv1) + 9(layer1) + 12(layer2) + 18(layer3) + 9(layer4) +1(fc)= ResNet50"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV0ZzsCdGhAi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e7686c84-6e55-4aed-dd2a-3fe7c27ae298"
      },
      "source": [
        "resnet50"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (layer1): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(64, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(16, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (3): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (4): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (5): Bottleneck(\n",
              "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): Bottleneck(\n",
              "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "    (2): Bottleneck(\n",
              "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hoTadQyoGiHb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "376e9de0-6084-444a-cbb5-bf17c619fb91"
      },
      "source": [
        "a=torch.Tensor(1,3,32,32).to(device)\n",
        "out = resnet50(a)\n",
        "print(out)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0338,  0.0122,  0.0168, -0.0428, -0.0382, -0.0435, -0.0441,  0.0176,\n",
            "         -0.0341, -0.0110]], device='cuda:0', grad_fn=<AddmmBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgvrgLv1GlMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().to(device)\n",
        "optimizer = torch.optim.SGD(resnet50.parameters(), lr = 0.1, momentum = 0.9, weight_decay=5e-4)\n",
        "lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aD-cilBOGm20",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc_check(net, test_set, epoch, save=0):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in test_set:\n",
        "            images, labels = data\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = net(images)\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    \n",
        "    acc = (100 * correct / total)\n",
        "    print('Accuracy of the network on the 10000 test images: %d %%' % acc)\n",
        "    if save:\n",
        "        torch.save(net.state_dict(), \"./model/model_epoch_{}_acc_{}.pth\".format(epoch, int(acc)))\n",
        "    return acc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6yCEXNEGqWE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf1b5127-11f0-4d0b-d106-3fe4296a1041"
      },
      "source": [
        "print(len(trainloader))\n",
        "epochs = 200\n",
        "\n",
        "for epoch in range(epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        " \n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = resnet50(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 30 == 29:    # print every 30 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 30))\n",
        "            running_loss = 0.0\n",
        "    lr_sche.step()\n",
        "    #Check Accuracy\n",
        "    acc = acc_check(resnet50, testloader, epoch, save=0)\n",
        "      \n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "196\n",
            "[1,    30] loss: 1.204\n",
            "[1,    60] loss: 1.149\n",
            "[1,    90] loss: 1.131\n",
            "[1,   120] loss: 1.109\n",
            "[1,   150] loss: 1.085\n",
            "[1,   180] loss: 1.052\n",
            "Accuracy of the network on the 10000 test images: 59 %\n",
            "[2,    30] loss: 0.992\n",
            "[2,    60] loss: 0.982\n",
            "[2,    90] loss: 0.958\n",
            "[2,   120] loss: 0.969\n",
            "[2,   150] loss: 0.929\n",
            "[2,   180] loss: 0.911\n",
            "Accuracy of the network on the 10000 test images: 61 %\n",
            "[3,    30] loss: 0.889\n",
            "[3,    60] loss: 0.853\n",
            "[3,    90] loss: 0.857\n",
            "[3,   120] loss: 0.857\n",
            "[3,   150] loss: 0.848\n",
            "[3,   180] loss: 0.828\n",
            "Accuracy of the network on the 10000 test images: 66 %\n",
            "[4,    30] loss: 0.812\n",
            "[4,    60] loss: 0.781\n",
            "[4,    90] loss: 0.773\n",
            "[4,   120] loss: 0.752\n",
            "[4,   150] loss: 0.754\n",
            "[4,   180] loss: 0.737\n",
            "Accuracy of the network on the 10000 test images: 67 %\n",
            "[5,    30] loss: 0.710\n",
            "[5,    60] loss: 0.696\n",
            "[5,    90] loss: 0.696\n",
            "[5,   120] loss: 0.666\n",
            "[5,   150] loss: 0.669\n",
            "[5,   180] loss: 0.673\n",
            "Accuracy of the network on the 10000 test images: 71 %\n",
            "[6,    30] loss: 0.608\n",
            "[6,    60] loss: 0.612\n",
            "[6,    90] loss: 0.628\n",
            "[6,   120] loss: 0.627\n",
            "[6,   150] loss: 0.616\n",
            "[6,   180] loss: 0.611\n",
            "Accuracy of the network on the 10000 test images: 72 %\n",
            "[7,    30] loss: 0.577\n",
            "[7,    60] loss: 0.546\n",
            "[7,    90] loss: 0.577\n",
            "[7,   120] loss: 0.605\n",
            "[7,   150] loss: 0.585\n",
            "[7,   180] loss: 0.567\n",
            "Accuracy of the network on the 10000 test images: 76 %\n",
            "[8,    30] loss: 0.536\n",
            "[8,    60] loss: 0.541\n",
            "[8,    90] loss: 0.561\n",
            "[8,   120] loss: 0.540\n",
            "[8,   150] loss: 0.518\n",
            "[8,   180] loss: 0.519\n",
            "Accuracy of the network on the 10000 test images: 74 %\n",
            "[9,    30] loss: 0.439\n",
            "[9,    60] loss: 0.388\n",
            "[9,    90] loss: 0.415\n",
            "[9,   120] loss: 0.410\n",
            "[9,   150] loss: 0.399\n",
            "[9,   180] loss: 0.409\n",
            "Accuracy of the network on the 10000 test images: 80 %\n",
            "[10,    30] loss: 0.361\n",
            "[10,    60] loss: 0.374\n",
            "[10,    90] loss: 0.390\n",
            "[10,   120] loss: 0.379\n",
            "[10,   150] loss: 0.411\n",
            "[10,   180] loss: 0.387\n",
            "Accuracy of the network on the 10000 test images: 80 %\n",
            "[11,    30] loss: 0.350\n",
            "[11,    60] loss: 0.371\n",
            "[11,    90] loss: 0.359\n",
            "[11,   120] loss: 0.379\n",
            "[11,   150] loss: 0.355\n",
            "[11,   180] loss: 0.376\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[12,    30] loss: 0.346\n",
            "[12,    60] loss: 0.339\n",
            "[12,    90] loss: 0.346\n",
            "[12,   120] loss: 0.366\n",
            "[12,   150] loss: 0.360\n",
            "[12,   180] loss: 0.371\n",
            "Accuracy of the network on the 10000 test images: 80 %\n",
            "[13,    30] loss: 0.339\n",
            "[13,    60] loss: 0.324\n",
            "[13,    90] loss: 0.363\n",
            "[13,   120] loss: 0.358\n",
            "[13,   150] loss: 0.348\n",
            "[13,   180] loss: 0.350\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[14,    30] loss: 0.335\n",
            "[14,    60] loss: 0.320\n",
            "[14,    90] loss: 0.333\n",
            "[14,   120] loss: 0.348\n",
            "[14,   150] loss: 0.344\n",
            "[14,   180] loss: 0.357\n",
            "Accuracy of the network on the 10000 test images: 82 %\n",
            "[15,    30] loss: 0.324\n",
            "[15,    60] loss: 0.336\n",
            "[15,    90] loss: 0.329\n",
            "[15,   120] loss: 0.336\n",
            "[15,   150] loss: 0.317\n",
            "[15,   180] loss: 0.354\n",
            "Accuracy of the network on the 10000 test images: 82 %\n",
            "[16,    30] loss: 0.302\n",
            "[16,    60] loss: 0.313\n",
            "[16,    90] loss: 0.320\n",
            "[16,   120] loss: 0.337\n",
            "[16,   150] loss: 0.351\n",
            "[16,   180] loss: 0.345\n",
            "Accuracy of the network on the 10000 test images: 82 %\n",
            "[17,    30] loss: 0.313\n",
            "[17,    60] loss: 0.304\n",
            "[17,    90] loss: 0.324\n",
            "[17,   120] loss: 0.334\n",
            "[17,   150] loss: 0.319\n",
            "[17,   180] loss: 0.318\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[18,    30] loss: 0.300\n",
            "[18,    60] loss: 0.290\n",
            "[18,    90] loss: 0.320\n",
            "[18,   120] loss: 0.315\n",
            "[18,   150] loss: 0.328\n",
            "[18,   180] loss: 0.332\n",
            "Accuracy of the network on the 10000 test images: 81 %\n",
            "[19,    30] loss: 0.253\n",
            "[19,    60] loss: 0.199\n",
            "[19,    90] loss: 0.210\n",
            "[19,   120] loss: 0.210\n",
            "[19,   150] loss: 0.210\n",
            "[19,   180] loss: 0.204\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[20,    30] loss: 0.178\n",
            "[20,    60] loss: 0.182\n",
            "[20,    90] loss: 0.179\n",
            "[20,   120] loss: 0.196\n",
            "[20,   150] loss: 0.185\n",
            "[20,   180] loss: 0.196\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[21,    30] loss: 0.160\n",
            "[21,    60] loss: 0.155\n",
            "[21,    90] loss: 0.173\n",
            "[21,   120] loss: 0.190\n",
            "[21,   150] loss: 0.190\n",
            "[21,   180] loss: 0.199\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[22,    30] loss: 0.164\n",
            "[22,    60] loss: 0.166\n",
            "[22,    90] loss: 0.161\n",
            "[22,   120] loss: 0.184\n",
            "[22,   150] loss: 0.191\n",
            "[22,   180] loss: 0.202\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[23,    30] loss: 0.157\n",
            "[23,    60] loss: 0.160\n",
            "[23,    90] loss: 0.161\n",
            "[23,   120] loss: 0.175\n",
            "[23,   150] loss: 0.179\n",
            "[23,   180] loss: 0.187\n",
            "Accuracy of the network on the 10000 test images: 82 %\n",
            "[24,    30] loss: 0.165\n",
            "[24,    60] loss: 0.169\n",
            "[24,    90] loss: 0.164\n",
            "[24,   120] loss: 0.177\n",
            "[24,   150] loss: 0.169\n",
            "[24,   180] loss: 0.192\n",
            "Accuracy of the network on the 10000 test images: 83 %\n",
            "[25,    30] loss: 0.164\n",
            "[25,    60] loss: 0.158\n",
            "[25,    90] loss: 0.159\n",
            "[25,   120] loss: 0.172\n",
            "[25,   150] loss: 0.175\n",
            "[25,   180] loss: 0.175\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[26,    30] loss: 0.154\n",
            "[26,    60] loss: 0.152\n",
            "[26,    90] loss: 0.162\n",
            "[26,   120] loss: 0.180\n",
            "[26,   150] loss: 0.192\n",
            "[26,   180] loss: 0.184\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[27,    30] loss: 0.153\n",
            "[27,    60] loss: 0.149\n",
            "[27,    90] loss: 0.171\n",
            "[27,   120] loss: 0.172\n",
            "[27,   150] loss: 0.154\n",
            "[27,   180] loss: 0.174\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[28,    30] loss: 0.177\n",
            "[28,    60] loss: 0.175\n",
            "[28,    90] loss: 0.163\n",
            "[28,   120] loss: 0.179\n",
            "[28,   150] loss: 0.167\n",
            "[28,   180] loss: 0.179\n",
            "Accuracy of the network on the 10000 test images: 84 %\n",
            "[29,    30] loss: 0.111\n",
            "[29,    60] loss: 0.101\n",
            "[29,    90] loss: 0.089\n",
            "[29,   120] loss: 0.080\n",
            "[29,   150] loss: 0.079\n",
            "[29,   180] loss: 0.078\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[30,    30] loss: 0.069\n",
            "[30,    60] loss: 0.064\n",
            "[30,    90] loss: 0.064\n",
            "[30,   120] loss: 0.072\n",
            "[30,   150] loss: 0.066\n",
            "[30,   180] loss: 0.062\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[31,    30] loss: 0.054\n",
            "[31,    60] loss: 0.058\n",
            "[31,    90] loss: 0.053\n",
            "[31,   120] loss: 0.062\n",
            "[31,   150] loss: 0.062\n",
            "[31,   180] loss: 0.058\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[32,    30] loss: 0.051\n",
            "[32,    60] loss: 0.054\n",
            "[32,    90] loss: 0.059\n",
            "[32,   120] loss: 0.058\n",
            "[32,   150] loss: 0.062\n",
            "[32,   180] loss: 0.057\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[33,    30] loss: 0.052\n",
            "[33,    60] loss: 0.053\n",
            "[33,    90] loss: 0.055\n",
            "[33,   120] loss: 0.052\n",
            "[33,   150] loss: 0.060\n",
            "[33,   180] loss: 0.060\n",
            "Accuracy of the network on the 10000 test images: 85 %\n",
            "[34,    30] loss: 0.054\n",
            "[34,    60] loss: 0.050\n",
            "[34,    90] loss: 0.052\n",
            "[34,   120] loss: 0.059\n",
            "[34,   150] loss: 0.064\n",
            "[34,   180] loss: 0.062\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[35,    30] loss: 0.055\n",
            "[35,    60] loss: 0.051\n",
            "[35,    90] loss: 0.052\n",
            "[35,   120] loss: 0.054\n",
            "[35,   150] loss: 0.058\n",
            "[35,   180] loss: 0.052\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[36,    30] loss: 0.056\n",
            "[36,    60] loss: 0.046\n",
            "[36,    90] loss: 0.050\n",
            "[36,   120] loss: 0.054\n",
            "[36,   150] loss: 0.059\n",
            "[36,   180] loss: 0.058\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[37,    30] loss: 0.067\n",
            "[37,    60] loss: 0.068\n",
            "[37,    90] loss: 0.073\n",
            "[37,   120] loss: 0.056\n",
            "[37,   150] loss: 0.057\n",
            "[37,   180] loss: 0.065\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[38,    30] loss: 0.060\n",
            "[38,    60] loss: 0.056\n",
            "[38,    90] loss: 0.071\n",
            "[38,   120] loss: 0.064\n",
            "[38,   150] loss: 0.060\n",
            "[38,   180] loss: 0.069\n",
            "Accuracy of the network on the 10000 test images: 85 %\n",
            "[39,    30] loss: 0.042\n",
            "[39,    60] loss: 0.032\n",
            "[39,    90] loss: 0.029\n",
            "[39,   120] loss: 0.027\n",
            "[39,   150] loss: 0.025\n",
            "[39,   180] loss: 0.026\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[40,    30] loss: 0.021\n",
            "[40,    60] loss: 0.018\n",
            "[40,    90] loss: 0.021\n",
            "[40,   120] loss: 0.018\n",
            "[40,   150] loss: 0.021\n",
            "[40,   180] loss: 0.019\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[41,    30] loss: 0.020\n",
            "[41,    60] loss: 0.018\n",
            "[41,    90] loss: 0.016\n",
            "[41,   120] loss: 0.016\n",
            "[41,   150] loss: 0.017\n",
            "[41,   180] loss: 0.018\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[42,    30] loss: 0.017\n",
            "[42,    60] loss: 0.013\n",
            "[42,    90] loss: 0.013\n",
            "[42,   120] loss: 0.017\n",
            "[42,   150] loss: 0.014\n",
            "[42,   180] loss: 0.012\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[43,    30] loss: 0.015\n",
            "[43,    60] loss: 0.013\n",
            "[43,    90] loss: 0.013\n",
            "[43,   120] loss: 0.016\n",
            "[43,   150] loss: 0.016\n",
            "[43,   180] loss: 0.015\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[44,    30] loss: 0.013\n",
            "[44,    60] loss: 0.013\n",
            "[44,    90] loss: 0.014\n",
            "[44,   120] loss: 0.013\n",
            "[44,   150] loss: 0.014\n",
            "[44,   180] loss: 0.015\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[45,    30] loss: 0.012\n",
            "[45,    60] loss: 0.012\n",
            "[45,    90] loss: 0.011\n",
            "[45,   120] loss: 0.012\n",
            "[45,   150] loss: 0.013\n",
            "[45,   180] loss: 0.010\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[46,    30] loss: 0.012\n",
            "[46,    60] loss: 0.011\n",
            "[46,    90] loss: 0.010\n",
            "[46,   120] loss: 0.009\n",
            "[46,   150] loss: 0.011\n",
            "[46,   180] loss: 0.011\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[47,    30] loss: 0.009\n",
            "[47,    60] loss: 0.008\n",
            "[47,    90] loss: 0.008\n",
            "[47,   120] loss: 0.010\n",
            "[47,   150] loss: 0.012\n",
            "[47,   180] loss: 0.011\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[48,    30] loss: 0.009\n",
            "[48,    60] loss: 0.009\n",
            "[48,    90] loss: 0.010\n",
            "[48,   120] loss: 0.010\n",
            "[48,   150] loss: 0.010\n",
            "[48,   180] loss: 0.009\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[49,    30] loss: 0.009\n",
            "[49,    60] loss: 0.008\n",
            "[49,    90] loss: 0.008\n",
            "[49,   120] loss: 0.010\n",
            "[49,   150] loss: 0.008\n",
            "[49,   180] loss: 0.007\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[50,    30] loss: 0.008\n",
            "[50,    60] loss: 0.006\n",
            "[50,    90] loss: 0.007\n",
            "[50,   120] loss: 0.008\n",
            "[50,   150] loss: 0.007\n",
            "[50,   180] loss: 0.007\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[51,    30] loss: 0.006\n",
            "[51,    60] loss: 0.006\n",
            "[51,    90] loss: 0.007\n",
            "[51,   120] loss: 0.006\n",
            "[51,   150] loss: 0.006\n",
            "[51,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[52,    30] loss: 0.007\n",
            "[52,    60] loss: 0.005\n",
            "[52,    90] loss: 0.007\n",
            "[52,   120] loss: 0.006\n",
            "[52,   150] loss: 0.007\n",
            "[52,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 87 %\n",
            "[53,    30] loss: 0.005\n",
            "[53,    60] loss: 0.006\n",
            "[53,    90] loss: 0.005\n",
            "[53,   120] loss: 0.005\n",
            "[53,   150] loss: 0.007\n",
            "[53,   180] loss: 0.006\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[54,    30] loss: 0.006\n",
            "[54,    60] loss: 0.005\n",
            "[54,    90] loss: 0.005\n",
            "[54,   120] loss: 0.006\n",
            "[54,   150] loss: 0.006\n",
            "[54,   180] loss: 0.005\n",
            "Accuracy of the network on the 10000 test images: 86 %\n",
            "[55,    30] loss: 0.004\n",
            "[55,    60] loss: 0.006\n",
            "[55,    90] loss: 0.004\n",
            "[55,   120] loss: 0.005\n",
            "[55,   150] loss: 0.005\n",
            "[55,   180] loss: 0.006\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-eafb46bdd670>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     97\u001b[0m                 \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m                     \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mmomentum\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0mparam_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sLcPu5jcGr-2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = resnet50(images)\n",
        "        \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        \n",
        "        total += labels.size(0)\n",
        "        \n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the network on the 10000 test images: %d %%' % (\n",
        "    100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}